{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Drug to Drug Interaction Multiclass Prediction\n",
        "\n",
        "This notebook contains experiments using Random Forest Classifier for multiclass classification. The data used is sourced from an online database called [DrugBank](https://go.drugbank.com/)."
      ],
      "metadata": {
        "id": "tQOrq5KOhmD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment decription\n",
        "\n",
        "### 1st Experiment\n",
        "  **Baseline model**\n",
        "\n",
        " A first look at the model's metrics without any data preprocessing or model fine tuning.\n",
        "\n",
        "### 2nd Experiment\n",
        "  - Data Preprocessing\n",
        "    - Upsampling minority classes\n",
        "    - Downsampling majority class\n",
        "  - Hyperparameter tuning (GridSearch)\n",
        "\n",
        "### 3rd Experiment\n",
        "  Same as experiment **2** , + balanced weights in classes as RFC parameters\n",
        "\n",
        "### 4rth Experiment\n",
        "\n",
        "  Same as experiment **2**, + experimenting with manual weights per class"
      ],
      "metadata": {
        "id": "TwFPq1yJigKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation"
      ],
      "metadata": {
        "id": "W_cSrs0Bk3ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import KFold, cross_val_predict, cross_val_score, StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix,classification_report, f1_score, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Mli5A8thBuj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/DDI/Development Files/FinalFeatures.csv')\n",
        "\n",
        "df['Description'] = df['Description'].replace(6, 5)\n",
        "\n",
        "# Keep column names\n",
        "column_names = df.columns[1:106]\n",
        "\n",
        "# Separate features from classes\n",
        "X = df.iloc[:, 1:106].values\n",
        "y = df.iloc[:, -1].values"
      ],
      "metadata": {
        "id": "g96C4TZmB_2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Experiment"
      ],
      "metadata": {
        "id": "reBQ8qI7mMJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a random forest classifier with 100 trees\n",
        "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Define a 10-fold cross-validation object\n",
        "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Get cross-validated predictions\n",
        "y_pred = cross_val_predict(rfc, X, y, cv=cv)\n",
        "\n",
        "# Get the unique class labels\n",
        "labels = np.unique(y)\n",
        "\n",
        "# Get the classification report for each class\n",
        "report = classification_report(y, y_pred, labels=labels)\n",
        "\n",
        "# Print the classification report\n",
        "print(report)"
      ],
      "metadata": {
        "id": "Jjf0PTSHJ8nS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/DDI/Development Files/FinalFeatures.csv')\n",
        "\n",
        "# Keep column names\n",
        "column_names = df.columns[1:106]\n",
        "df['Description'] = df['Description'].replace(6, 5)\n",
        "\n",
        "# Separate features from classes\n",
        "X1 = df.iloc[:, 1:106].values\n",
        "y1 = df.iloc[:, -1].values\n",
        "\n",
        "X, X_test, y, y_test = train_test_split(X1, y1, test_size=0.2, stratify = y1 )"
      ],
      "metadata": {
        "id": "IvVV8j-4aSSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2nd Experiment"
      ],
      "metadata": {
        "id": "F3A77wAcp3ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a random forest classifier\n",
        "rfc = RandomForestClassifier()\n",
        "\n",
        "# Define parameters for grid search\n",
        "param_grid = {\n",
        "    'n_estimators': [75, 100, 125],\n",
        "    'max_depth': [10, 20],\n",
        "    'min_samples_leaf': [5, 10]\n",
        "}\n",
        "\n",
        "# Define upsampling ratios to try\n",
        "sampling_values = [[2,10], [2,5], [3,10], [3,5], [1,10], [1,5]]\n",
        "\n",
        "# Create a stratified 10-fold cross-validation object\n",
        "outer_cv = StratifiedKFold(n_splits=10)\n",
        "inner_cv = StratifiedKFold(n_splits=5)\n",
        "\n",
        "# Initialize variables to track the best overall model\n",
        "best_score = 0\n",
        "best_model = None\n",
        "best_cm_outer = None\n",
        "best_report_outer = None\n",
        "outer_scores_list = []\n",
        "\n",
        "# Initialize variables to accumulate confusion matrices\n",
        "overall_cm = np.zeros((len(np.unique(y)), len(np.unique(y))), dtype=int)\n",
        "overall_supports = np.zeros(len(np.unique(y)), dtype=int)\n",
        "\n",
        "# Initialize a dictionary to store cumulative recalls for each class\n",
        "cumulative_recalls = {class_label: 0 for class_label in np.unique(y)}\n",
        "cumulative_precisions = {class_label: 0 for class_label in np.unique(y)}\n",
        "cumulative_f1_scores= {class_label: 0 for class_label in np.unique(y)}\n",
        "\n",
        "# Initialize a dictionary to store the count of occurrences of each class\n",
        "class_counts = {class_label: 0 for class_label in np.unique(y)}\n",
        "\n",
        "# Iterate over the outer cross-validation splits\n",
        "for train_index, test_index in outer_cv.split(X, y):\n",
        "    X_train_outer, X_test_outer = X[train_index], X[test_index]\n",
        "    y_train_outer, y_test_outer = y[train_index], y[test_index]\n",
        "\n",
        "    # Initialize variables to track the best inner model\n",
        "    best_inner_score = 0\n",
        "    best_inner_model = None\n",
        "    best_cm_inner = None\n",
        "    best_report_inner = None\n",
        "\n",
        "    feature_names = df.columns[1:106].astype(str)\n",
        "    #print(feature_names)\n",
        "\n",
        "    # Turn numpy arrays into dfs\n",
        "    X_train_df = pd.DataFrame(X, columns=column_names)\n",
        "    y_train_df = pd.DataFrame(y, columns=['Description'])\n",
        "\n",
        "    # Combine X_train and y_train for later use\n",
        "    train_data = pd.concat([X_train_df, y_train_df], axis=1)\n",
        "\n",
        "    for i,j in sampling_values:\n",
        "        #for upsampling_ratio in upsampling_ratios:\n",
        "        majority_class_0 = train_data[train_data['Description'] == 0]\n",
        "        minority_class_1 = train_data[train_data['Description'] == 1]\n",
        "        minority_class_3 = train_data[train_data['Description'] == 3]\n",
        "        class_2 = train_data[train_data['Description'] == 2]\n",
        "        class_4 = train_data[train_data['Description'] == 4]\n",
        "        class_5 = train_data[train_data['Description'] == 5]\n",
        "\n",
        "        # Calculate the number of samples based on percentages\n",
        "        majority_downsampled_samples = len(majority_class_0) // i  # Use // for integer division\n",
        "        minority_upsampled_samples_1 = int(len(minority_class_1) * j)  # Convert to integer\n",
        "        minority_upsampled_samples_3 = int(len(minority_class_3) * j)  # Convert to integer\n",
        "\n",
        "\n",
        "        # Downsample the majority class 0\n",
        "        majority_downsampled = resample(majority_class_0,\n",
        "                                replace=False,\n",
        "                                n_samples=majority_downsampled_samples)\n",
        "\n",
        "        # Combine minority classes 1 and 3\n",
        "        minority_combined = pd.concat([minority_class_1, minority_class_3])\n",
        "\n",
        "        # Upsample minority classes 1 and 3 to have 1000 examples each using SMOTE\n",
        "        smote = SMOTE(sampling_strategy={1: minority_upsampled_samples_1, 3: minority_upsampled_samples_3})\n",
        "        minority_upsampled, y1 = smote.fit_resample(minority_combined.iloc[:, :105], minority_combined.iloc[:, -1])\n",
        "        minority_upsampled = pd.DataFrame(minority_upsampled, columns=minority_combined.columns[:105])\n",
        "        minority_upsampled['Description'] = y1\n",
        "\n",
        "        # Combine classes\n",
        "        df_downsampled = pd.concat([majority_downsampled, minority_upsampled, class_2, class_4, class_5], ignore_index=True)\n",
        "\n",
        "        # Shuffle the dataset\n",
        "        df_downsampled = df_downsampled.sample(frac=1, random_state=42)\n",
        "\n",
        "        # Split the data back into X_train and y_train\n",
        "        X_train_upsampled = df_downsampled.iloc[:, 0:105].values\n",
        "        y_train_upsampled = df_downsampled.iloc[:, -1].values\n",
        "\n",
        "        # Create a grid search object for hyperparameter tuning\n",
        "        grid_search_inner = GridSearchCV(rfc, param_grid=param_grid, cv=inner_cv,\n",
        "                                         scoring='f1_macro', refit=True)\n",
        "\n",
        "        # Fit the grid search to the upsampled training data\n",
        "        grid_search_inner.fit(X_train_upsampled, y_train_upsampled)\n",
        "\n",
        "        # Get the best model from the inner grid search\n",
        "        best_inner_model = grid_search_inner.best_estimator_\n",
        "\n",
        "    #get metrics for outer CV\n",
        "    y_pred_outer = best_inner_model.predict(X_test)\n",
        "    outer_score = classification_report(y_test, y_pred_outer, labels=np.unique(y), output_dict=True)\n",
        "    cm_outer = confusion_matrix(y_test, y_pred_outer, labels=np.unique(y))\n",
        "\n",
        "    # Accumulate the confusion matrix\n",
        "    overall_cm += cm_outer\n",
        "    overall_supports += np.sum(cm_outer, axis=1)\n",
        "\n",
        "    outer_scores_list.append(outer_score)\n",
        "\n",
        "    # Iterate over the outer_scores_list\n",
        "    for outer_score in outer_scores_list:\n",
        "        # Iterate over the classes in the outer_score dictionary\n",
        "        for class_label in np.unique(y):\n",
        "            if class_label not in ['accuracy', 'macro avg', 'weighted avg']:\n",
        "                avg_recall_score = outer_score.get(str(class_label), {'recall': 0})['recall']\n",
        "                avg_precision_score = outer_score.get(str(class_label), {'precision': 0})['precision']\n",
        "                avg_f1_score = outer_score.get(str(class_label), {'f1-score': 0})['f1-score']\n",
        "\n",
        "                support = outer_score.get(str(class_label), {'support': 0})['support']\n",
        "\n",
        "                # Update cumulative recalls and class counts\n",
        "                cumulative_recalls[class_label] += avg_recall_score\n",
        "                cumulative_precisions[class_label] += avg_precision_score\n",
        "                cumulative_f1_scores[class_label] += avg_f1_score\n",
        "                class_counts[class_label] += 1\n",
        "\n",
        "# Print or use the overall confusion matrix\n",
        "print(\"Overall Confusion Matrix:\")\n",
        "print(overall_cm)\n",
        "\n",
        "# Calculate the average recall for each class\n",
        "average_recalls = {class_label: cumulative_recall / class_counts[class_label] for class_label, cumulative_recall in cumulative_recalls.items()}\n",
        "average_precisions = {class_label: cumulative_precision / class_counts[class_label] for class_label, cumulative_precision in cumulative_precisions.items()}\n",
        "average_f1_scores = {class_label: cumulative_f1_score / class_counts[class_label] for class_label, cumulative_f1_score in cumulative_f1_scores.items()}\n",
        "\n",
        "# Print the average metrics for each class\n",
        "for class_label in np.unique(y):\n",
        "    if class_label not in ['accuracy', 'macro avg', 'weighted avg']:\n",
        "        print(f\"Class '{class_label}':\")\n",
        "        print(f\"Average F1 Score: {average_f1_scores[class_label]}\")\n",
        "        print(f\"Average Recall: {average_recalls[class_label]}\")\n",
        "        print(f\"Average Precision: {average_precisions[class_label]}\")\n"
      ],
      "metadata": {
        "id": "N0Ni4wq8Jw7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3rd Experiment"
      ],
      "metadata": {
        "id": "WmmVaA8V0Dma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a random forest classifier\n",
        "rfc = RandomForestClassifier(class_weight = \"balanced\")\n",
        "\n",
        "# Define parameters for grid search\n",
        "param_grid = {\n",
        "    'n_estimators': [75, 100, 125],\n",
        "    'max_depth': [10, 20],\n",
        "    'min_samples_leaf': [5, 10]\n",
        "}\n",
        "\n",
        "# Define upsampling ratios to try\n",
        "sampling_values = [[2,10], [2,5], [3,10], [3,5], [1,10], [1,5]]\n",
        "\n",
        "# Create a stratified 10-fold cross-validation object\n",
        "outer_cv = StratifiedKFold(n_splits=10)\n",
        "inner_cv = StratifiedKFold(n_splits=5)\n",
        "\n",
        "# Initialize variables to track the best overall model\n",
        "best_score = 0\n",
        "best_model = None\n",
        "best_cm_outer = None\n",
        "best_report_outer = None\n",
        "outer_scores_list = []\n",
        "\n",
        "# Initialize variables to accumulate confusion matrices\n",
        "overall_cm = np.zeros((len(np.unique(y)), len(np.unique(y))), dtype=int)\n",
        "overall_supports = np.zeros(len(np.unique(y)), dtype=int)\n",
        "\n",
        "# Initialize a dictionary to store cumulative recalls for each class\n",
        "cumulative_recalls = {class_label: 0 for class_label in np.unique(y)}\n",
        "cumulative_precisions = {class_label: 0 for class_label in np.unique(y)}\n",
        "cumulative_f1_scores= {class_label: 0 for class_label in np.unique(y)}\n",
        "\n",
        "# Initialize a dictionary to store the count of occurrences of each class\n",
        "class_counts = {class_label: 0 for class_label in np.unique(y)}\n",
        "\n",
        "# Iterate over the outer cross-validation splits\n",
        "for train_index, test_index in outer_cv.split(X, y):\n",
        "    X_train_outer, X_test_outer = X[train_index], X[test_index]\n",
        "    y_train_outer, y_test_outer = y[train_index], y[test_index]\n",
        "\n",
        "    # Initialize variables to track the best inner model\n",
        "    best_inner_score = 0\n",
        "    best_inner_model = None\n",
        "    best_cm_inner = None\n",
        "    best_report_inner = None\n",
        "\n",
        "    feature_names = df.columns[1:106].astype(str)\n",
        "\n",
        "    # Turn numpy arrays into dfs\n",
        "    X_train_df = pd.DataFrame(X, columns=column_names)\n",
        "    y_train_df = pd.DataFrame(y, columns=['Description'])\n",
        "\n",
        "    # Combine X_train and y_train for later use\n",
        "    train_data = pd.concat([X_train_df, y_train_df], axis=1)\n",
        "\n",
        "    for i,j in sampling_values:\n",
        "        #for upsampling_ratio in upsampling_ratios:\n",
        "        majority_class_0 = train_data[train_data['Description'] == 0]\n",
        "        minority_class_1 = train_data[train_data['Description'] == 1]\n",
        "        minority_class_3 = train_data[train_data['Description'] == 3]\n",
        "        class_2 = train_data[train_data['Description'] == 2]\n",
        "        class_4 = train_data[train_data['Description'] == 4]\n",
        "        class_5 = train_data[train_data['Description'] == 5]\n",
        "\n",
        "        # Calculate the number of samples based on percentages\n",
        "        majority_downsampled_samples = len(majority_class_0) // i  # Use // for integer division\n",
        "        minority_upsampled_samples_1 = int(len(minority_class_1) * j)  # Convert to integer\n",
        "        minority_upsampled_samples_3 = int(len(minority_class_3) * j)  # Convert to integer\n",
        "\n",
        "\n",
        "        # Downsample the majority class 0\n",
        "        majority_downsampled = resample(majority_class_0,\n",
        "                                replace=False,\n",
        "                                n_samples=majority_downsampled_samples)\n",
        "\n",
        "        # Combine minority classes 1 and 3\n",
        "        minority_combined = pd.concat([minority_class_1, minority_class_3])\n",
        "\n",
        "        # Upsample minority classes 1 and 3 to have 1000 examples each using SMOTE\n",
        "        smote = SMOTE(sampling_strategy={1: minority_upsampled_samples_1, 3: minority_upsampled_samples_3})\n",
        "        minority_upsampled, y1 = smote.fit_resample(minority_combined.iloc[:, :105], minority_combined.iloc[:, -1])\n",
        "        minority_upsampled = pd.DataFrame(minority_upsampled, columns=minority_combined.columns[:105])\n",
        "        minority_upsampled['Description'] = y1\n",
        "\n",
        "        # Combine classes\n",
        "        df_downsampled = pd.concat([majority_downsampled, minority_upsampled, class_2, class_4, class_5], ignore_index=True)\n",
        "\n",
        "        # Shuffle the dataset\n",
        "        df_downsampled = df_downsampled.sample(frac=1, random_state=42)\n",
        "\n",
        "        # Split the data back into X_train and y_train\n",
        "        X_train_upsampled = df_downsampled.iloc[:, 0:105].values\n",
        "        y_train_upsampled = df_downsampled.iloc[:, -1].values\n",
        "\n",
        "        # Create a grid search object for hyperparameter tuning\n",
        "        grid_search_inner = GridSearchCV(rfc, param_grid=param_grid, cv=inner_cv,\n",
        "                                         scoring='f1_macro', refit=True)\n",
        "\n",
        "        # Fit the grid search to the upsampled training data\n",
        "        grid_search_inner.fit(X_train_upsampled, y_train_upsampled)\n",
        "\n",
        "        # Get the best model from the inner grid search\n",
        "        best_inner_model = grid_search_inner.best_estimator_\n",
        "\n",
        "    #get metrics for outer CV\n",
        "    y_pred_outer = best_inner_model.predict(X_test)\n",
        "    outer_score = classification_report(y_test, y_pred_outer, labels=np.unique(y), output_dict=True)\n",
        "    cm_outer = confusion_matrix(y_test, y_pred_outer, labels=np.unique(y))\n",
        "\n",
        "    # Accumulate the confusion matrix\n",
        "    overall_cm += cm_outer\n",
        "    overall_supports += np.sum(cm_outer, axis=1)\n",
        "\n",
        "    outer_scores_list.append(outer_score)\n",
        "\n",
        "    # Iterate over the outer_scores_list\n",
        "    for outer_score in outer_scores_list:\n",
        "        # Iterate over the classes in the outer_score dictionary\n",
        "        for class_label in np.unique(y):\n",
        "            if class_label not in ['accuracy', 'macro avg', 'weighted avg']:\n",
        "                avg_recall_score = outer_score.get(str(class_label), {'recall': 0})['recall']\n",
        "                avg_precision_score = outer_score.get(str(class_label), {'precision': 0})['precision']\n",
        "                avg_f1_score = outer_score.get(str(class_label), {'f1-score': 0})['f1-score']\n",
        "                support = outer_score.get(str(class_label), {'support': 0})['support']\n",
        "\n",
        "                # Update cumulative recalls and class counts\n",
        "                cumulative_recalls[class_label] += avg_recall_score\n",
        "                cumulative_precisions[class_label] += avg_precision_score\n",
        "                cumulative_f1_scores[class_label] += avg_f1_score\n",
        "                class_counts[class_label] += 1\n",
        "\n",
        "# Print or use the overall confusion matrix\n",
        "print(\"Overall Confusion Matrix:\")\n",
        "print(overall_cm)\n",
        "\n",
        "# Calculate the average recall for each class\n",
        "average_recalls = {class_label: cumulative_recall / class_counts[class_label] for class_label, cumulative_recall in cumulative_recalls.items()}\n",
        "average_precisions = {class_label: cumulative_precision / class_counts[class_label] for class_label, cumulative_precision in cumulative_precisions.items()}\n",
        "average_f1_scores = {class_label: cumulative_f1_score / class_counts[class_label] for class_label, cumulative_f1_score in cumulative_f1_scores.items()}\n",
        "\n",
        "# Print the average metrics for each class\n",
        "for class_label in np.unique(y):\n",
        "    if class_label not in ['accuracy', 'macro avg', 'weighted avg']:\n",
        "        print(f\"Class '{class_label}':\")\n",
        "        print(f\"Average F1 Score: {average_f1_scores[class_label]}\")\n",
        "        print(f\"Average Recall: {average_recalls[class_label]}\")\n",
        "        print(f\"Average Precision: {average_precisions[class_label]}\")\n"
      ],
      "metadata": {
        "id": "zT-fEaFbbGzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4th Experiment"
      ],
      "metadata": {
        "id": "DkemAKsI13so"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict_weights = {0:0.4, 1: 4, 2: 1.3, 3: 2.3, 4: 2, 5: 3}\n",
        "\n",
        "# Create a random forest classifier\n",
        "rfc = RandomForestClassifier(class_weight = dict_weights)\n",
        "\n",
        "# Define parameters for grid search\n",
        "param_grid = {\n",
        "    'n_estimators': [75, 100, 125],\n",
        "    'max_depth': [10, 20],\n",
        "    'min_samples_leaf': [5, 10]\n",
        "}\n",
        "\n",
        "# Define upsampling ratios to try\n",
        "sampling_values = [[2,10], [2,5], [3,10], [3,5], [1,10], [1,5]]\n",
        "\n",
        "\n",
        "# Create a stratified 10-fold cross-validation object\n",
        "outer_cv = StratifiedKFold(n_splits=10)\n",
        "inner_cv = StratifiedKFold(n_splits=5)\n",
        "\n",
        "# Initialize variables to track the best overall model\n",
        "best_score = 0\n",
        "best_model = None\n",
        "best_cm_outer = None\n",
        "best_report_outer = None\n",
        "outer_scores_list = []\n",
        "\n",
        "# Initialize variables to accumulate confusion matrices\n",
        "overall_cm = np.zeros((len(np.unique(y)), len(np.unique(y))), dtype=int)\n",
        "overall_supports = np.zeros(len(np.unique(y)), dtype=int)\n",
        "\n",
        "# Initialize a dictionary to store cumulative recalls for each class\n",
        "cumulative_recalls = {class_label: 0 for class_label in np.unique(y)}\n",
        "cumulative_precisions = {class_label: 0 for class_label in np.unique(y)}\n",
        "cumulative_f1_scores= {class_label: 0 for class_label in np.unique(y)}\n",
        "\n",
        "# Initialize a dictionary to store the count of occurrences of each class\n",
        "class_counts = {class_label: 0 for class_label in np.unique(y)}\n",
        "\n",
        "# Iterate over the outer cross-validation splits\n",
        "for train_index, test_index in outer_cv.split(X, y):\n",
        "    X_train_outer, X_test_outer = X[train_index], X[test_index]\n",
        "    y_train_outer, y_test_outer = y[train_index], y[test_index]\n",
        "\n",
        "    # Initialize variables to track the best inner model\n",
        "    best_inner_score = 0\n",
        "    best_inner_model = None\n",
        "    best_cm_inner = None\n",
        "    best_report_inner = None\n",
        "\n",
        "    feature_names = df.columns[1:106].astype(str)\n",
        "    #print(feature_names)\n",
        "\n",
        "    # Turn numpy arrays into dfs\n",
        "    X_train_df = pd.DataFrame(X, columns=column_names)\n",
        "    y_train_df = pd.DataFrame(y, columns=['Description'])\n",
        "\n",
        "    # Combine X_train and y_train for later use\n",
        "    train_data = pd.concat([X_train_df, y_train_df], axis=1)\n",
        "\n",
        "    for i,j in sampling_values:\n",
        "        #for upsampling_ratio in upsampling_ratios:\n",
        "        majority_class_0 = train_data[train_data['Description'] == 0]\n",
        "        minority_class_1 = train_data[train_data['Description'] == 1]\n",
        "        minority_class_3 = train_data[train_data['Description'] == 3]\n",
        "        class_2 = train_data[train_data['Description'] == 2]\n",
        "        class_4 = train_data[train_data['Description'] == 4]\n",
        "        class_5 = train_data[train_data['Description'] == 5]\n",
        "\n",
        "        # Calculate the number of samples based on percentages\n",
        "        majority_downsampled_samples = len(majority_class_0) // i  # Use // for integer division\n",
        "        minority_upsampled_samples_1 = int(len(minority_class_1) * j)  # Convert to integer\n",
        "        minority_upsampled_samples_3 = int(len(minority_class_3) * j)  # Convert to integer\n",
        "\n",
        "\n",
        "        # Downsample the majority class 0\n",
        "        majority_downsampled = resample(majority_class_0,\n",
        "                                replace=False,\n",
        "                                n_samples=majority_downsampled_samples)\n",
        "\n",
        "        # Combine minority classes 1 and 3\n",
        "        minority_combined = pd.concat([minority_class_1, minority_class_3])\n",
        "\n",
        "        # Upsample minority classes 1 and 3 to have 1000 examples each using SMOTE\n",
        "        smote = SMOTE(sampling_strategy={1: minority_upsampled_samples_1, 3: minority_upsampled_samples_3})\n",
        "        minority_upsampled, y1 = smote.fit_resample(minority_combined.iloc[:, :105], minority_combined.iloc[:, -1])\n",
        "        minority_upsampled = pd.DataFrame(minority_upsampled, columns=minority_combined.columns[:105])\n",
        "        minority_upsampled['Description'] = y1\n",
        "\n",
        "        # Combine classes\n",
        "        df_downsampled = pd.concat([majority_downsampled, minority_upsampled, class_2, class_4, class_5], ignore_index=True)\n",
        "\n",
        "        # Shuffle the dataset\n",
        "        df_downsampled = df_downsampled.sample(frac=1, random_state=42)\n",
        "\n",
        "        # Split the data back into X_train and y_train\n",
        "        X_train_upsampled = df_downsampled.iloc[:, 0:105].values\n",
        "        y_train_upsampled = df_downsampled.iloc[:, -1].values\n",
        "\n",
        "        # Create a grid search object for hyperparameter tuning\n",
        "        grid_search_inner = GridSearchCV(rfc, param_grid=param_grid, cv=inner_cv,\n",
        "                                         scoring='f1_macro', refit=True)\n",
        "\n",
        "        # Fit the grid search to the upsampled training data\n",
        "        grid_search_inner.fit(X_train_upsampled, y_train_upsampled)\n",
        "\n",
        "        # Get the best model from the inner grid search\n",
        "        best_inner_model = grid_search_inner.best_estimator_\n",
        "\n",
        "    #get metrics for outer CV\n",
        "    y_pred_outer = best_inner_model.predict(X_test)\n",
        "    outer_score = classification_report(y_test, y_pred_outer, labels=np.unique(y), output_dict=True)\n",
        "    cm_outer = confusion_matrix(y_test, y_pred_outer, labels=np.unique(y))\n",
        "\n",
        "    # Accumulate the confusion matrix\n",
        "    overall_cm += cm_outer\n",
        "    overall_supports += np.sum(cm_outer, axis=1)\n",
        "\n",
        "    outer_scores_list.append(outer_score)\n",
        "\n",
        "\n",
        "    # Iterate over the outer_scores_list\n",
        "    for outer_score in outer_scores_list:\n",
        "        # Iterate over the classes in the outer_score dictionary\n",
        "        for class_label in np.unique(y):\n",
        "            if class_label not in ['accuracy', 'macro avg', 'weighted avg']:\n",
        "                avg_recall_score = outer_score.get(str(class_label), {'recall': 0})['recall']\n",
        "                avg_precision_score = outer_score.get(str(class_label), {'precision': 0})['precision']\n",
        "                avg_f1_score = outer_score.get(str(class_label), {'f1-score': 0})['f1-score']\n",
        "\n",
        "                support = outer_score.get(str(class_label), {'support': 0})['support']\n",
        "\n",
        "                # Update cumulative recalls and class counts\n",
        "                cumulative_recalls[class_label] += avg_recall_score\n",
        "                cumulative_precisions[class_label] += avg_precision_score\n",
        "                cumulative_f1_scores[class_label] += avg_f1_score\n",
        "                class_counts[class_label] += 1\n",
        "\n",
        "# Print or use the overall confusion matrix\n",
        "print(\"Overall Confusion Matrix:\")\n",
        "print(overall_cm)\n",
        "\n",
        "\n",
        "# Calculate the average recall for each class\n",
        "average_recalls = {class_label: cumulative_recall / class_counts[class_label] for class_label, cumulative_recall in cumulative_recalls.items()}\n",
        "average_precisions = {class_label: cumulative_precision / class_counts[class_label] for class_label, cumulative_precision in cumulative_precisions.items()}\n",
        "average_f1_scores = {class_label: cumulative_f1_score / class_counts[class_label] for class_label, cumulative_f1_score in cumulative_f1_scores.items()}\n",
        "\n",
        "# Print the average metrics for each class\n",
        "for class_label in np.unique(y):\n",
        "    if class_label not in ['accuracy', 'macro avg', 'weighted avg']:\n",
        "        print(f\"Class '{class_label}':\")\n",
        "        print(f\"Average F1 Score: {average_f1_scores[class_label]}\")\n",
        "        print(f\"Average Recall: {average_recalls[class_label]}\")\n",
        "        print(f\"Average Precision: {average_precisions[class_label]}\")"
      ],
      "metadata": {
        "id": "gZOS4nJAMFIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict new DDIs and Feature Importance"
      ],
      "metadata": {
        "id": "qnka0fvDQDr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explainability & Predictions (Balanced Weights)"
      ],
      "metadata": {
        "id": "JAVnWTxTPWR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        " # Load the dataset\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/DDI/Development Files/FinalFeatures.csv')\n",
        "\n",
        "# Keep column names\n",
        "column_names = train_data.columns[1:106]\n",
        "train_data['Description'] = train_data['Description'].replace(6, 5)\n",
        "\n",
        "# Assuming 'train_data' is your original dataset\n",
        "# Step 1: Split data into train (80%) and test (20%), ensuring the test set contains only Class 0\n",
        "class_0_data = train_data[train_data['Description'] == 0]\n",
        "class_non_0_data = train_data[train_data['Description'] != 0]\n",
        "\n",
        "train_class_0, test_class_0 = train_test_split(class_0_data, test_size=0.2, random_state=42)\n",
        "\n",
        "train_data_final = pd.concat([train_class_0, class_non_0_data], ignore_index=True)\n",
        "\n",
        "# Step 2: Perform up/downsampling\n",
        "sampling_values = [[2, 10]]\n",
        "for i, j in sampling_values:\n",
        "    majority_class_0 = train_data_final[train_data_final['Description'] == 0]\n",
        "    minority_class_1 = train_data_final[train_data_final['Description'] == 1]\n",
        "    minority_class_3 = train_data_final[train_data_final['Description'] == 3]\n",
        "    class_2 = train_data_final[train_data_final['Description'] == 2]\n",
        "    class_4 = train_data_final[train_data_final['Description'] == 4]\n",
        "    class_5 = train_data_final[train_data_final['Description'] == 5]\n",
        "\n",
        "    # Downsample Class 0\n",
        "    majority_downsampled_samples = len(majority_class_0) // i\n",
        "    majority_downsampled = resample(majority_class_0, replace=False, n_samples=majority_downsampled_samples)\n",
        "\n",
        "    # Upsample Class 1 and 3 using SMOTE\n",
        "    minority_combined = pd.concat([minority_class_1, minority_class_3])\n",
        "    minority_upsampled_samples_1 = int(len(minority_class_1) * j)\n",
        "    minority_upsampled_samples_3 = int(len(minority_class_3) * j)\n",
        "\n",
        "    smote = SMOTE(sampling_strategy={1: minority_upsampled_samples_1, 3: minority_upsampled_samples_3})\n",
        "    minority_upsampled, y1 = smote.fit_resample(minority_combined.iloc[:, 1:105], minority_combined.iloc[:, -1])\n",
        "\n",
        "    minority_upsampled = pd.DataFrame(minority_upsampled, columns=minority_combined.columns[1:105])\n",
        "    minority_upsampled['Description'] = y1\n",
        "\n",
        "    # Combine all classes\n",
        "    df_final = pd.concat([majority_downsampled, minority_upsampled, class_2, class_4, class_5], ignore_index=True)\n",
        "\n",
        "    # Shuffle the dataset\n",
        "    df_final = df_final.sample(frac=1, random_state=42)\n",
        "\n",
        "# Step 3: Prepare training and test sets\n",
        "X_train = df_final.iloc[:, 1:106].values\n",
        "y_train = df_final.iloc[:, -1].values\n",
        "X_test = test_class_0.iloc[:, 1:106].values\n",
        "y_test = test_class_0.iloc[:, -1].values  # Should all be 0\n",
        "\n",
        "# Step 4: Train the Random Forest Classifier\n",
        "rfc = RandomForestClassifier(class_weight=\"balanced\", n_estimators=75, max_depth=10, min_samples_leaf=5, random_state=42)\n",
        "rfc.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the test set\n",
        "y_pred_proba = rfc.predict_proba(X_test)  # Get probabilities\n",
        "y_pred = rfc.predict(X_test)  # Get class predictions\n",
        "\n",
        "# Step 6: Identify misclassified examples (i.e., classified as anything other than 0)\n",
        "classified_indices = np.where(y_pred != 0)[0]\n",
        "classified_samples = test_class_0.iloc[classified_indices].copy()\n",
        "classified_samples['Predicted_Class'] = y_pred[classified_indices]\n",
        "classified_samples['Confidence'] = np.max(y_pred_proba[classified_indices], axis=1)  # Max confidence per row\n",
        "\n",
        "# Step 7: Extract feature importances\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': train_data.columns[:105],\n",
        "    'Importance': rfc.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Sort misclassified examples by confidence in descending order\n",
        "classified_samples = classified_samples.sort_values(by='Confidence', ascending=False)\n",
        "\n",
        "# Display misclassified examples with Drug Pair names\n",
        "print(\"\\nClassified Examples with Drug Pairs (Sorted by Confidence):\")\n",
        "print(classified_samples[['DB_PAIR', 'Predicted_Class', 'Confidence']])\n",
        "\n",
        "# Get the best (highest confidence) prediction for each class\n",
        "best_predictions_per_class = classified_samples.loc[\n",
        "    classified_samples.groupby('Predicted_Class')['Confidence'].idxmax()\n",
        "]\n",
        "\n",
        "# Display the best misclassified example for each class\n",
        "print(\"\\nBest Prediction for Each Class:\")\n",
        "print(best_predictions_per_class[['DB_PAIR', 'Predicted_Class', 'Confidence']])\n",
        "\n",
        "# Save it as a CSV file for further analysis\n",
        "best_predictions_per_class[['DB_PAIR', 'Predicted_Class', 'Confidence']].to_csv(\"best_predictions_per_class.csv\", index=False)\n",
        "\n",
        "# Display top 10 important features\n",
        "print(\"\\nTop 10 Feature Importances:\")\n",
        "print(feature_importance.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oV-C9OdAMLL",
        "outputId": "e0138cd9-9016-4902-85aa-eded34630b40"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-95f70ccb75bc>:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  minority_upsampled['Description'] = y1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classified Examples with Drug Pairs (Sorted by Confidence):\n",
            "               DB_PAIR  Predicted_Class  Confidence\n",
            "27922  DB00399_DB01181                2    0.699794\n",
            "11775  DB00399_DB00762                2    0.663858\n",
            "16374  DB00599_DB13145                5    0.660697\n",
            "20551  DB00293_DB00317                2    0.655927\n",
            "23276  DB00515_DB00646                5    0.651843\n",
            "...                ...              ...         ...\n",
            "24186  DB00762_DB01626                5    0.233621\n",
            "12383  DB00762_DB00821                5    0.229726\n",
            "15982  DB01143_DB04978                4    0.222120\n",
            "15498  DB00563_DB00765                5    0.221127\n",
            "8169   DB00563_DB01034                5    0.207922\n",
            "\n",
            "[1425 rows x 3 columns]\n",
            "\n",
            "Best Prediction for Each Class:\n",
            "               DB_PAIR  Predicted_Class  Confidence\n",
            "14857  DB00278_DB00530                1    0.338608\n",
            "27922  DB00399_DB01181                2    0.699794\n",
            "10980  DB00530_DB06688                3    0.373695\n",
            "26525  DB01229_DB06803                4    0.557237\n",
            "16374  DB00599_DB13145                5    0.660697\n",
            "\n",
            "Top 10 Feature Importances:\n",
            "                  Feature  Importance\n",
            "104     rel3_MENTIONED_IN    0.218807\n",
            "69      rel2_MENTIONED_IN    0.030658\n",
            "73   rel3_ASSOCIATED_WITH    0.025979\n",
            "87              rel3_IS_A    0.019388\n",
            "68              rel2_USES    0.018080\n",
            "62          rel2_PREVENTS    0.017929\n",
            "53               rel2_ISA    0.017557\n",
            "33              rel1_USES    0.017453\n",
            "101       rel3_STIMULATES    0.017348\n",
            "38   rel2_ASSOCIATED_WITH    0.016470\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explainability & Predictions (Custom Weights)"
      ],
      "metadata": {
        "id": "W7o7A8pZIe2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        " # Load the dataset\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/DDI/Development Files/FinalFeatures.csv')  # use your file path\n",
        "\n",
        "# Keep column names\n",
        "column_names = train_data.columns[1:106]\n",
        "train_data['Description'] = train_data['Description'].replace(6, 5)\n",
        "\n",
        "\n",
        "# Step 1: Split data into train (80%) and test (20%), ensuring the test set contains only Class 0\n",
        "class_0_data = train_data[train_data['Description'] == 0]\n",
        "class_non_0_data = train_data[train_data['Description'] != 0]\n",
        "\n",
        "train_class_0, test_class_0 = train_test_split(class_0_data, test_size=0.2, random_state=42)\n",
        "\n",
        "train_data_final = pd.concat([train_class_0, class_non_0_data], ignore_index=True)\n",
        "\n",
        "# Step 2: Perform up/downsampling\n",
        "sampling_values = [[2, 10]]\n",
        "for i, j in sampling_values:\n",
        "    majority_class_0 = train_data_final[train_data_final['Description'] == 0]\n",
        "    minority_class_1 = train_data_final[train_data_final['Description'] == 1]\n",
        "    minority_class_3 = train_data_final[train_data_final['Description'] == 3]\n",
        "    class_2 = train_data_final[train_data_final['Description'] == 2]\n",
        "    class_4 = train_data_final[train_data_final['Description'] == 4]\n",
        "    class_5 = train_data_final[train_data_final['Description'] == 5]\n",
        "\n",
        "    # Downsample Class 0\n",
        "    majority_downsampled_samples = len(majority_class_0) // i\n",
        "    majority_downsampled = resample(majority_class_0, replace=False, n_samples=majority_downsampled_samples)\n",
        "\n",
        "    # Upsample Class 1 and 3 using SMOTE\n",
        "    minority_combined = pd.concat([minority_class_1, minority_class_3])\n",
        "    minority_upsampled_samples_1 = int(len(minority_class_1) * j)\n",
        "    minority_upsampled_samples_3 = int(len(minority_class_3) * j)\n",
        "\n",
        "    smote = SMOTE(sampling_strategy={1: minority_upsampled_samples_1, 3: minority_upsampled_samples_3})\n",
        "    minority_upsampled, y1 = smote.fit_resample(minority_combined.iloc[:, 1:105], minority_combined.iloc[:, -1])\n",
        "\n",
        "    minority_upsampled = pd.DataFrame(minority_upsampled, columns=minority_combined.columns[1:105])\n",
        "    minority_upsampled['Description'] = y1\n",
        "\n",
        "    # Combine all classes\n",
        "    df_final = pd.concat([majority_downsampled, minority_upsampled, class_2, class_4, class_5], ignore_index=True)\n",
        "\n",
        "    # Shuffle the dataset\n",
        "    df_final = df_final.sample(frac=1, random_state=42)\n",
        "\n",
        "# Step 3: Prepare training and test sets\n",
        "X_train = df_final.iloc[:, 1:106].values\n",
        "y_train = df_final.iloc[:, -1].values\n",
        "X_test = test_class_0.iloc[:, 1:106].values\n",
        "y_test = test_class_0.iloc[:, -1].values  # Should all be 0\n",
        "\n",
        "# Step 4: Train the Random Forest Classifier\n",
        "dict_weights = {0: 0.4, 1: 4, 2: 1.3, 3: 2.3, 4: 2, 5: 3}\n",
        "rfc = RandomForestClassifier(class_weight=dict_weights, n_estimators=75, max_depth=10, min_samples_leaf=5, random_state=42)\n",
        "rfc.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the test set\n",
        "y_pred_proba = rfc.predict_proba(X_test)  # Get probabilities\n",
        "y_pred = rfc.predict(X_test)  # Get class predictions\n",
        "\n",
        "# Step 6: Identify misclassified examples (i.e., classified as anything other than 0)\n",
        "classified_indices = np.where(y_pred != 0)[0]\n",
        "classified_samples = test_class_0.iloc[classified_indices].copy()\n",
        "classified_samples['Predicted_Class'] = y_pred[classified_indices]\n",
        "classified_samples['Confidence'] = np.max(y_pred_proba[classified_indices], axis=1)  # Max confidence per row\n",
        "\n",
        "# Step 7: Extract feature importances\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': train_data.columns[:105],\n",
        "    'Importance': rfc.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Sort misclassified examples by confidence in descending order\n",
        "classified_samples = classified_samples.sort_values(by='Confidence', ascending=False)\n",
        "\n",
        "# Display misclassified examples with Drug Pair names\n",
        "print(\"\\nClassified Examples with Drug Pairs (Sorted by Confidence):\")\n",
        "print(classified_samples[['DB_PAIR', 'Predicted_Class', 'Confidence']])\n",
        "\n",
        "# Get the best (highest confidence) prediction for each class\n",
        "best_predictions_per_class = classified_samples.loc[\n",
        "    classified_samples.groupby('Predicted_Class')['Confidence'].idxmax()\n",
        "]\n",
        "\n",
        "# Display the best misclassified example for each class\n",
        "print(\"\\nBest Prediction for Each Class:\")\n",
        "print(best_predictions_per_class[['DB_PAIR', 'Predicted_Class', 'Confidence']])\n",
        "\n",
        "# Save it as a CSV file for further analysis\n",
        "best_predictions_per_class[['DB_PAIR', 'Predicted_Class', 'Confidence']].to_csv(\"best_predictions_per_class.csv\", index=False)\n",
        "\n",
        "# Display top 10 important features\n",
        "print(\"\\nTop 10 Feature Importances:\")\n",
        "print(feature_importance.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4MUJ6PJAlWn",
        "outputId": "8b27c6ba-17ec-41eb-9f1c-dea111b37bc1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-0727c4143ae2>:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  minority_upsampled['Description'] = y1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classified Examples with Drug Pairs (Sorted by Confidence):\n",
            "               DB_PAIR  Predicted_Class  Confidence\n",
            "16374  DB00599_DB13145                5    0.702814\n",
            "26525  DB01229_DB06803                4    0.658919\n",
            "12065  DB00426_DB13145                5    0.655771\n",
            "12715  DB06767_DB09063                4    0.655422\n",
            "23479  DB00557_DB13145                5    0.653597\n",
            "...                ...              ...         ...\n",
            "3790   DB06235_DB12465                4    0.263053\n",
            "18018  DB00361_DB09352                4    0.258857\n",
            "7269   DB08865_DB11989                5    0.251181\n",
            "5189   DB00970_DB09037                4    0.249755\n",
            "18767  DB08916_DB13025                5    0.245560\n",
            "\n",
            "[2071 rows x 3 columns]\n",
            "\n",
            "Best Prediction for Each Class:\n",
            "               DB_PAIR  Predicted_Class  Confidence\n",
            "14857  DB00278_DB00530                1    0.340146\n",
            "15551  DB00099_DB00642                2    0.533017\n",
            "10980  DB00530_DB06688                3    0.333969\n",
            "26525  DB01229_DB06803                4    0.658919\n",
            "16374  DB00599_DB13145                5    0.702814\n",
            "\n",
            "Top 10 Feature Importances:\n",
            "                  Feature  Importance\n",
            "104     rel3_MENTIONED_IN    0.244436\n",
            "69      rel2_MENTIONED_IN    0.031849\n",
            "73   rel3_ASSOCIATED_WITH    0.026264\n",
            "87              rel3_IS_A    0.023415\n",
            "62          rel2_PREVENTS    0.021594\n",
            "53               rel2_ISA    0.019593\n",
            "101       rel3_STIMULATES    0.016315\n",
            "75            rel3_CAUSES    0.015544\n",
            "55        rel2_lower_than    0.015362\n",
            "38   rel2_ASSOCIATED_WITH    0.015110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CCfR6gY7I42o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}